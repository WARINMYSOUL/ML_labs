{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3efc3f76",
   "metadata": {},
   "source": [
    "# Лабораторная работа 5: Seq2Seq-чатбот на RNN (LSTM/GRU)\n",
    "\n",
    "**Выполнил:** *Лунев Александр Вячеславович*  \n",
    "**Группа:** *СП1*  \n",
    "\n",
    "Цель работы — построить и обучить модель Seq2Seq с рекуррентными слоями (LSTM или GRU) \n",
    "для генерации ответов на русском языке в формате диалога.\n",
    "\n",
    "Основные шаги:\n",
    "1. Загрузка и первичный анализ датасета Toloka Persona Chat (русская версия).\n",
    "2. Разбор диалогов и формирование пар «реплика → ответ».\n",
    "3. Токенизация текста, построение словаря и подготовка тензоров.\n",
    "4. Реализация модели Seq2Seq: кодировщик (encoder) и декодер (decoder) с механизмом внимания.\n",
    "5. Обучение модели на обучающей выборке и оценка на валидации.\n",
    "6. Реализация генерации ответов (greedy и beam search).\n",
    "7. Простой интерактивный чат с обученной моделью.\n",
    "8. Обсуждение примеров диалогов и выводы.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d52c4f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется устройство: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Фиксируем зерно генераторов случайных чисел для воспроизводимости\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Выбираем устройство (GPU, если доступно, иначе CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используется устройство: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8300b4",
   "metadata": {},
   "source": [
    "## 1. Загрузка и первичный анализ датасета\n",
    "\n",
    "В данной работе используется датасет Toloka Persona Chat (русская версия).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34a6d6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые строки датафрейма:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>persona_1_profile</th>\n",
       "      <th>persona_2_profile</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;span class=participant_1&gt;У меня любимая работ...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Ищу принца.&lt;br /&gt;Вед...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Пользователь 2: Прив...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;span class=participant_1&gt;Я работаю учителем&lt;b...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Я бизнесмен&lt;br /&gt;У м...</td>\n",
       "      <td>&lt;span class=participant_1&gt;Пользователь 1: Прив...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;span class=participant_1&gt;Я купила дом&lt;br /&gt;Я ...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Я пою в караоке&lt;br /...</td>\n",
       "      <td>&lt;span class=participant_1&gt;Пользователь 1: Прив...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;span class=participant_1&gt;я врач и женат&lt;br /&gt;...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Я мальчик&lt;br /&gt;Я учу...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Пользователь 2: Здра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;span class=participant_1&gt;Я школьница.&lt;br /&gt;Я ...</td>\n",
       "      <td>&lt;span class=participant_2&gt;Я простоват.&lt;br /&gt;Лю...</td>\n",
       "      <td>&lt;span class=participant_1&gt;Пользователь 1: Прив...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   persona_1_profile  \\\n",
       "0  <span class=participant_1>У меня любимая работ...   \n",
       "1  <span class=participant_1>Я работаю учителем<b...   \n",
       "2  <span class=participant_1>Я купила дом<br />Я ...   \n",
       "3  <span class=participant_1>я врач и женат<br />...   \n",
       "4  <span class=participant_1>Я школьница.<br />Я ...   \n",
       "\n",
       "                                   persona_2_profile  \\\n",
       "0  <span class=participant_2>Ищу принца.<br />Вед...   \n",
       "1  <span class=participant_2>Я бизнесмен<br />У м...   \n",
       "2  <span class=participant_2>Я пою в караоке<br /...   \n",
       "3  <span class=participant_2>Я мальчик<br />Я учу...   \n",
       "4  <span class=participant_2>Я простоват.<br />Лю...   \n",
       "\n",
       "                                            dialogue  \n",
       "0  <span class=participant_2>Пользователь 2: Прив...  \n",
       "1  <span class=participant_1>Пользователь 1: Прив...  \n",
       "2  <span class=participant_1>Пользователь 1: Прив...  \n",
       "3  <span class=participant_2>Пользователь 2: Здра...  \n",
       "4  <span class=participant_1>Пользователь 1: Прив...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Названия колонок:\n",
      "['persona_1_profile', 'persona_2_profile', 'dialogue']\n",
      "\n",
      "Количество строк в датасете: 10013\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"data/dialogues.tsv\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, sep=\"\\t\")\n",
    "\n",
    "print(\"Первые строки датафрейма:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nНазвания колонок:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "DIALOGUE_COLUMN = \"dialogue\"  # при необходимости измените название\n",
    "\n",
    "if DIALOGUE_COLUMN not in df.columns:\n",
    "    raise ValueError(\n",
    "        f\"В датафрейме нет колонки '{DIALOGUE_COLUMN}'. \"\n",
    "        f\"Доступные колонки: {df.columns.tolist()}. \"\n",
    "        f\"Поставьте сюда правильное имя столбца.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nКоличество строк в датасете: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa283e5b",
   "metadata": {},
   "source": [
    "## 2. Разбор диалогов и формирование пар «реплика → ответ»\n",
    "\n",
    "На этом этапе:\n",
    "1. Очищаем текст (нижний регистр, удаление HTML-тегов, ссылок и лишних пробелов).\n",
    "2. Разбиваем диалог на отдельные реплики.\n",
    "   - В некоторых версиях датасета реплики разделены строками (`\\n`),\n",
    "     в других — разделителем `|||`. Код пытается обработать оба варианта.\n",
    "3. Фильтруем слишком короткие реплики.\n",
    "4. Формируем пары вида:  \n",
    "   *реплика_i → реплика_(i+1)* для всех соседних реплик в диалоге.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431cba46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество сформированных пар 'реплика → ответ': 1487\n",
      "\n",
      "Примеры пар:\n",
      "\n",
      "Пара 1:\n",
      "  Вход:  \" пользователь 1: привет пользователь 2: привет пользователь 2: как дела? ты откуда? пользователь 1: хорошо! а у тебя? пользователь 1: днепр! а ты? пользователь 2: хорошо, я из ялуторовска, это в россии, тюменская область пользователь 1: далеко\n",
      "  Ответ: кем работаешь? пользователь 2: работаю на себя))) пользователь 2: а ты? пользователь 1: а я работаю фрилансером пользователь 2: что делаешь в свободное время? пользователь 1: чем занимаешься в свободное время? пользователь 2: :) пользователь 1: саморазвиваюсть, ценю время м не трачу его попусту пользователь 1: а ты? пользователь 2: люблю погулять в парке ... саморазвитие это всегда хорошо пользователь 1: ещё я придумываю что-то ное, чтоб удивить людей пользователь 1: новое* пользователь 2: нравиться делать людей счастливее пользователь 1: именно так \"\n",
      "\n",
      "Пара 2:\n",
      "  Вход:  \" пользователь 2: здравствуйте пользователь 1: привет пользователь 2: как твои дела? пользователь 1: хорошо пользователь 1: сколько тебе лет? пользователь 2: мне 21, а тебе? пользователь 1: мне 65 пользователь 1: кем работаешь? пользователь 1: кем работаешь? пользователь 2: я работаю фрилансером пользователь 1: я учителем пользователь 1: живу у моря пользователь 1: у тебя есть дети? пользователь 2: здорово пользователь 2: нет, я совсем одинока пользователь 1: у меня дочка в садик ходит пользователь 2: сколько дочке? пользователь 1: 5 пользователь 1: я ей не давно сшил шил носки люблю вязать пользователь 1: особенно спицами пользователь 1: а ты,? пользователь 2: я не люблю вязать\n",
      "  Ответ: я люблю стрелять и хочу ружьё пользователь 1: ты женщина ? пользователь 1: или нет пользователь 2: да пользователь 2: а ты? пользователь 1: нет противоположно пользователь 1: хорошая беседа, пока!!! пользователь 2: поняла пользователь 2: пока \"\n",
      "\n",
      "Пара 3:\n",
      "  Вход:  \" пользователь 1: приветствую пользователь 2: привет пользователь 1: ты кто? пользователь 2: маша\n",
      "  Ответ: кем роботаеш? пользователь 1: я аня -певица пользователь 2: я библиотекарь пользователь 2: живу с крисой пользователь 1: давно работаешь? нравится пользователь 1: у меня попугай пользователь 2: да пользователь 1: дети есть? пользователь 2: какое тбое хобби? пользователь 2: нет пользователь 1: чем увлекаешься? пользователь 2: увлекаюсь космосом пользователь 1: у меня сын есть. взрослый уже пользователь 2: ясно \"\n"
     ]
    }
   ],
   "source": [
    "MIN_UTTER_LEN = 2  # минимальное количество слов в реплике, чтобы её учитывать\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_dialogue(raw_dialogue: str):\n",
    "    if not isinstance(raw_dialogue, str):\n",
    "        return []\n",
    "\n",
    "    s = raw_dialogue.strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    if \"|||\" in s:\n",
    "        parts = s.split(\"|||\")\n",
    "    elif \"\\n\" in s:\n",
    "        parts = s.split(\"\\n\")\n",
    "    elif \"\\\\n\" in s:\n",
    "        parts = s.split(\"\\\\n\")\n",
    "    else:\n",
    "        parts = [s]\n",
    "\n",
    "    parts = [p.strip() for p in parts if p.strip()]\n",
    "    return parts\n",
    "\n",
    "\n",
    "def tokenize(text: str):\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def build_pairs(dialogues, min_utter_len: int = MIN_UTTER_LEN, max_pairs: int | None = None):\n",
    "    pairs = []\n",
    "\n",
    "    for raw_dialogue in dialogues:\n",
    "        utterances = split_dialogue(raw_dialogue)\n",
    "        # очищаем и фильтруем по длине\n",
    "        utterances = [clean_text(u) for u in utterances]\n",
    "        utterances = [u for u in utterances if len(tokenize(u)) >= min_utter_len]\n",
    "\n",
    "        for i in range(len(utterances) - 1):\n",
    "            src = utterances[i]\n",
    "            tgt = utterances[i + 1]\n",
    "            pairs.append((src, tgt))\n",
    "\n",
    "            if max_pairs is not None and len(pairs) >= max_pairs:\n",
    "                return pairs\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "pairs = build_pairs(df[DIALOGUE_COLUMN].tolist(), min_utter_len=MIN_UTTER_LEN)\n",
    "\n",
    "print(f\"Количество сформированных пар 'реплика → ответ': {len(pairs)}\")\n",
    "print(\"\\nПримеры пар:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nПара {i+1}:\")\n",
    "    print(f\"  Вход:  {pairs[i][0]}\")\n",
    "    print(f\"  Ответ: {pairs[i][1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1064cb",
   "metadata": {},
   "source": [
    "## 3. Токенизация, построение словаря и подготовка тензоров\n",
    "\n",
    "На этом шаге:\n",
    "1. Строим словарь по всем словам из входов и ответов.\n",
    "2. Вводим специальные токены:\n",
    "   - `<pad>` — добивка до нужной длины;\n",
    "   - `<bos>` — начало ответа;\n",
    "   - `<eos>` — конец ответа;\n",
    "   - `<unk>` — неизвестное слово.\n",
    "3. Ограничиваем размер словаря (например, 20 000 наиболее частых слов).\n",
    "4. Преобразуем текстовые реплики в последовательности индексов фиксированной длины\n",
    "   для входа и выхода:\n",
    "   - вход: `src_ids`;\n",
    "   - вход декодера (сдвиг вправо с BOS): `tgt_input_ids`;\n",
    "   - цель (с EOS в конце): `tgt_output_ids`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d5d69fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря (включая спец. токены): 20000\n",
      "Количество последовательностей: 1487\n",
      "\n",
      "Пример преобразования:\n",
      "Исходная реплика:  \" пользователь 1: привет пользователь 2: привет пользователь 2: как дела? ты откуда? пользователь 1: хорошо! а у тебя? пользователь 1: днепр! а ты? пользователь 2: хорошо, я из ялуторовска, это в россии, тюменская область пользователь 1: далеко\n",
      "Ответ:            кем работаешь? пользователь 2: работаю на себя))) пользователь 2: а ты? пользователь 1: а я работаю фрилансером пользователь 2: что делаешь в свободное время? пользователь 1: чем занимаешься в свободное время? пользователь 2: :) пользователь 1: саморазвиваюсть, ценю время м не трачу его попусту пользователь 1: а ты? пользователь 2: люблю погулять в парке ... саморазвитие это всегда хорошо пользователь 1: ещё я придумываю что-то ное, чтоб удивить людей пользователь 1: новое* пользователь 2: нравиться делать людей счастливее пользователь 1: именно так \"\n",
      "src_ids:          [18, 4, 5, 36, 4, 6, 36, 4, 6, 17, 89, 15, 216, 4, 5, 902, 8, 10, 72, 4]\n",
      "tgt_input_ids:    [1, 69, 73, 4, 6, 40, 14, 11373, 4, 6, 8, 34, 4, 5, 8, 7, 40, 4517, 4, 6]\n",
      "tgt_output_ids:   [69, 73, 4, 6, 40, 14, 11373, 4, 6, 8, 34, 4, 5, 8, 7, 40, 4517, 4, 6, 2]\n",
      "Восстановленный ответ:  кем работаешь? пользователь 2: работаю на себя))) пользователь 2: а ты? пользователь 1: а я работаю фрилансером пользователь 2:\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 20000\n",
    "MAX_LEN_SRC = 20\n",
    "MAX_LEN_TGT = 20\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, BOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
    "\n",
    "counter = collections.Counter()\n",
    "for src, tgt in pairs:\n",
    "    counter.update(tokenize(src))\n",
    "    counter.update(tokenize(tgt))\n",
    "\n",
    "most_common_words = [w for w, c in counter.most_common(MAX_VOCAB_SIZE - len(SPECIAL_TOKENS))]\n",
    "\n",
    "word2index: dict[str, int] = {}\n",
    "index2word: dict[int, str] = {}\n",
    "\n",
    "for idx, token in enumerate(SPECIAL_TOKENS + most_common_words):\n",
    "    word2index[token] = idx\n",
    "    index2word[idx] = token\n",
    "\n",
    "PAD_IDX = word2index[PAD_TOKEN]\n",
    "BOS_IDX = word2index[BOS_TOKEN]\n",
    "EOS_IDX = word2index[EOS_TOKEN]\n",
    "UNK_IDX = word2index[UNK_TOKEN]\n",
    "\n",
    "vocab_size = len(word2index)\n",
    "print(f\"Размер словаря (включая спец. токены): {vocab_size}\")\n",
    "\n",
    "\n",
    "def text_to_indices(\n",
    "    text: str,\n",
    "    word2index: dict[str, int],\n",
    "    max_len: int,\n",
    "    add_bos: bool = False,\n",
    "    add_eos: bool = False,\n",
    ") -> list[int]:\n",
    "    tokens = tokenize(text)\n",
    "    ids: list[int] = []\n",
    "\n",
    "    if add_bos:\n",
    "        ids.append(BOS_IDX)\n",
    "\n",
    "    for tok in tokens:\n",
    "        ids.append(word2index.get(tok, UNK_IDX))\n",
    "        if len(ids) >= max_len - int(add_eos):\n",
    "            break\n",
    "\n",
    "    if add_eos:\n",
    "        ids.append(EOS_IDX)\n",
    "\n",
    "    if len(ids) < max_len:\n",
    "        ids += [PAD_IDX] * (max_len - len(ids))\n",
    "    else:\n",
    "        ids = ids[:max_len]\n",
    "\n",
    "    return ids\n",
    "\n",
    "\n",
    "def indices_to_text(indices: list[int], index2word: dict[int, str]) -> str:\n",
    "    words: list[str] = []\n",
    "    for idx in indices:\n",
    "        if idx == PAD_IDX:\n",
    "            continue\n",
    "        if idx == BOS_IDX:\n",
    "            continue\n",
    "        if idx == EOS_IDX:\n",
    "            break\n",
    "        words.append(index2word.get(idx, UNK_TOKEN))\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "src_sequences: list[list[int]] = []\n",
    "tgt_input_sequences: list[list[int]] = []\n",
    "tgt_output_sequences: list[list[int]] = []\n",
    "\n",
    "for src, tgt in pairs:\n",
    "    src_ids = text_to_indices(src, word2index, MAX_LEN_SRC, add_bos=False, add_eos=False)\n",
    "    tgt_input_ids = text_to_indices(tgt, word2index, MAX_LEN_TGT, add_bos=True, add_eos=False)\n",
    "    tgt_output_ids = text_to_indices(tgt, word2index, MAX_LEN_TGT, add_bos=False, add_eos=True)\n",
    "\n",
    "    src_sequences.append(src_ids)\n",
    "    tgt_input_sequences.append(tgt_input_ids)\n",
    "    tgt_output_sequences.append(tgt_output_ids)\n",
    "\n",
    "print(f\"Количество последовательностей: {len(src_sequences)}\")\n",
    "\n",
    "# Покажем пример преобразования\n",
    "example_idx = 0\n",
    "print(\"\\nПример преобразования:\")\n",
    "print(\"Исходная реплика: \", pairs[example_idx][0])\n",
    "print(\"Ответ:           \", pairs[example_idx][1])\n",
    "print(\"src_ids:         \", src_sequences[example_idx])\n",
    "print(\"tgt_input_ids:   \", tgt_input_sequences[example_idx])\n",
    "print(\"tgt_output_ids:  \", tgt_output_sequences[example_idx])\n",
    "print(\"Восстановленный ответ: \", indices_to_text(tgt_output_sequences[example_idx], index2word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39c8f9",
   "metadata": {},
   "source": [
    "## 4. Разбиение на обучающую и валидационную выборки, DataLoader\n",
    "\n",
    "Теперь:\n",
    "1. Делим все пары на обучающую и валидационную части (например, 90% / 10%).\n",
    "2. Создаём класс `Dataset` для PyTorch.\n",
    "3. Создаём `DataLoader` для удобной итерации по батчам.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e339980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: 1338\n",
      "Размер валидационной выборки: 149\n"
     ]
    }
   ],
   "source": [
    "indices = list(range(len(src_sequences)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_size = int(0.9 * len(indices))\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "\n",
    "def select_by_indices(sequences: list[list[int]], idxs: list[int]) -> list[list[int]]:\n",
    "    return [sequences[i] for i in idxs]\n",
    "\n",
    "\n",
    "train_src = select_by_indices(src_sequences, train_indices)\n",
    "train_tgt_in = select_by_indices(tgt_input_sequences, train_indices)\n",
    "train_tgt_out = select_by_indices(tgt_output_sequences, train_indices)\n",
    "\n",
    "val_src = select_by_indices(src_sequences, val_indices)\n",
    "val_tgt_in = select_by_indices(tgt_input_sequences, val_indices)\n",
    "val_tgt_out = select_by_indices(tgt_output_sequences, val_indices)\n",
    "\n",
    "\n",
    "class DialogueDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_sequences: list[list[int]],\n",
    "        tgt_input_sequences: list[list[int]],\n",
    "        tgt_output_sequences: list[list[int]],\n",
    "    ):\n",
    "        assert len(src_sequences) == len(tgt_input_sequences) == len(tgt_output_sequences)\n",
    "        self.src_sequences = src_sequences\n",
    "        self.tgt_input_sequences = tgt_input_sequences\n",
    "        self.tgt_output_sequences = tgt_output_sequences\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.src_sequences)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        src = torch.tensor(self.src_sequences[idx], dtype=torch.long)\n",
    "        tgt_in = torch.tensor(self.tgt_input_sequences[idx], dtype=torch.long)\n",
    "        tgt_out = torch.tensor(self.tgt_output_sequences[idx], dtype=torch.long)\n",
    "        return src, tgt_in, tgt_out\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = DialogueDataset(train_src, train_tgt_in, train_tgt_out)\n",
    "val_dataset = DialogueDataset(val_src, val_tgt_in, val_tgt_out)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Размер обучающей выборки: {len(train_dataset)}\")\n",
    "print(f\"Размер валидационной выборки: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea9d437",
   "metadata": {},
   "source": [
    "## 5. Определение модели Seq2Seq (энкодер, декодер и внимание)\n",
    "\n",
    "Здесь реализуем:\n",
    "1. **Кодировщик (Encoder)** на базе RNN (GRU или LSTM):\n",
    "   - получает на вход последовательность индексов,\n",
    "   - возвращает последовательность скрытых состояний и финальное скрытое состояние.\n",
    "2. **Механизм внимания (Luong attention)**:\n",
    "   - на каждом шаге декодера вычисляет веса по всем выходам кодировщика,\n",
    "   - формирует контекстный вектор.\n",
    "3. **Декодер (Decoder) с вниманием**:\n",
    "   - на вход получает текущий токен, скрытое состояние и выходы кодировщика,\n",
    "   - с помощью внимания собирает контекст и предсказывает следующий токен.\n",
    "4. **Общий класс Seq2Seq**, объединяющий энкодер и декодер.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "766ce91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0.1, rnn_type=\"gru\"):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=PAD_IDX)\n",
    "        rnn_type = rnn_type.lower()\n",
    "        if rnn_type == \"lstm\":\n",
    "            self.rnn = nn.LSTM(\n",
    "                embed_size,\n",
    "                hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        else:\n",
    "            self.rnn = nn.GRU(\n",
    "                embed_size,\n",
    "                hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        self.rnn_type = rnn_type\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)  # [batch, src_len, embed_size]\n",
    "        encoder_outputs, hidden = self.rnn(embedded)\n",
    "        return encoder_outputs, hidden\n",
    "\n",
    "\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.linear_in = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs, mask=None):\n",
    "        dec = self.linear_in(decoder_hidden)\n",
    "        dec = dec.unsqueeze(2)\n",
    "\n",
    "        energies = torch.bmm(encoder_outputs, dec).squeeze(2)\n",
    "\n",
    "        if mask is not None:\n",
    "            energies = energies.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attn_weights = self.softmax(energies)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        return context, attn_weights\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1, dropout=0.1, rnn_type=\"gru\"):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=PAD_IDX)\n",
    "        self.rnn_type = rnn_type.lower()\n",
    "        if self.rnn_type == \"lstm\":\n",
    "            self.rnn = nn.LSTM(\n",
    "                embed_size,\n",
    "                hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        else:\n",
    "            self.rnn = nn.GRU(\n",
    "                embed_size,\n",
    "                hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "\n",
    "        self.attention = LuongAttention(hidden_size)\n",
    "        self.fc_concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_size = vocab_size\n",
    "\n",
    "    def forward(self, input_step, hidden, encoder_outputs, mask=None):\n",
    "        embedded = self.embedding(input_step).unsqueeze(1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        outputs, hidden = self.rnn(embedded, hidden)\n",
    "        decoder_hidden = outputs.squeeze(1)\n",
    "\n",
    "        context, attn_weights = self.attention(decoder_hidden, encoder_outputs, mask)\n",
    "\n",
    "        concat_input = torch.cat([decoder_hidden, context], dim=-1)\n",
    "        concat_output = torch.tanh(self.fc_concat(concat_input))\n",
    "\n",
    "        logits = self.fc_out(concat_output)\n",
    "\n",
    "        return logits, hidden, attn_weights\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, tgt_input, teacher_forcing_ratio=0.5):\n",
    "        batch_size, tgt_len = tgt_input.size()\n",
    "        vocab_size = self.decoder.output_size\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_len, vocab_size, device=self.device)\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        src_mask = (src != PAD_IDX)\n",
    "\n",
    "        input_step = tgt_input[:, 0]  # [batch]\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            logits, hidden, attn_weights = self.decoder(input_step, hidden,\n",
    "                                                        encoder_outputs, src_mask)\n",
    "            outputs[:, t, :] = logits\n",
    "\n",
    "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "            top1 = logits.argmax(dim=-1)  # [batch]\n",
    "            input_step = tgt_input[:, t] if use_teacher_forcing else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51f8a2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(20000, 300, padding_idx=0)\n",
      "    (rnn): LSTM(300, 768, num_layers=2, batch_first=True, dropout=0.1)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(20000, 300, padding_idx=0)\n",
      "    (rnn): LSTM(300, 768, num_layers=2, batch_first=True, dropout=0.1)\n",
      "    (attention): LuongAttention(\n",
      "      (linear_in): Linear(in_features=768, out_features=768, bias=False)\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (fc_concat): Linear(in_features=1536, out_features=768, bias=True)\n",
      "    (fc_out): Linear(in_features=768, out_features=20000, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Всего параметров: 45.17 млн\n"
     ]
    }
   ],
   "source": [
    "EMBED_SIZE = 300\n",
    "HIDDEN_SIZE = 768\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "RNN_TYPE = \"lstm\"  # можно поменять на \"lstm\" для экспериментов\n",
    "\n",
    "encoder = Encoder(vocab_size, EMBED_SIZE, HIDDEN_SIZE,\n",
    "                  num_layers=NUM_LAYERS, dropout=DROPOUT, rnn_type=RNN_TYPE)\n",
    "decoder = Decoder(vocab_size, EMBED_SIZE, HIDDEN_SIZE,\n",
    "                  num_layers=NUM_LAYERS, dropout=DROPOUT, rnn_type=RNN_TYPE)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(model)\n",
    "print(f\"Всего параметров: {total_params/1e6:.2f} млн\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b792bd4",
   "metadata": {},
   "source": [
    "## 6. Функция потерь, оптимизатор и цикл обучения\n",
    "\n",
    "Для обучения:\n",
    "- используем кросс-энтропийную функцию потерь (CrossEntropyLoss),\n",
    "  игнорируя позиции с токеном `<pad>`,\n",
    "- оптимизатор Adam,\n",
    "- teacher forcing для ускорения сходимости.\n",
    "\n",
    "На каждом шаге:\n",
    "1. Прогоняем модель на батче `src`, `tgt_input`.\n",
    "2. Считаем loss между предсказаниями и `tgt_output` (со сдвигом на один шаг).\n",
    "3. Обновляем веса.\n",
    "4. Периодически считаем loss на валидационной выборке.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d069ed1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 01: train_loss = 0.8431, val_loss = 8.2789\n",
      "  → Новая лучшая модель сохранена (val_loss = 8.2789)\n",
      "Эпоха 02: train_loss = 0.5171, val_loss = 8.4029\n",
      "  → Нет улучшения на валидации 1/5 эпох(и)\n",
      "Эпоха 03: train_loss = 0.3349, val_loss = 8.6222\n",
      "  → Нет улучшения на валидации 2/5 эпох(и)\n",
      "Эпоха 04: train_loss = 0.2603, val_loss = 8.7166\n",
      "  → Нет улучшения на валидации 3/5 эпох(и)\n",
      "Эпоха 05: train_loss = 0.2104, val_loss = 8.9048\n",
      "  → Нет улучшения на валидации 4/5 эпох(и)\n",
      "\n",
      "Загружена лучшая модель с val_loss = 8.2789\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели с сохранением лучшей версии и ранней остановкой\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 5\n",
    "TEACHER_FORCING_RATIO_TRAIN = 0.5\n",
    "TEACHER_FORCING_RATIO_VAL = 1.0\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "PATIENCE = 5\n",
    "BEST_MODEL_PATH = \"best_seq2seq.pt\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs_without_improve = 0\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for src, tgt_input, tgt_output in train_loader:\n",
    "        src = src.to(device)\n",
    "        tgt_input = tgt_input.to(device)\n",
    "        tgt_output = tgt_output.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(src, tgt_input, teacher_forcing_ratio=TEACHER_FORCING_RATIO_TRAIN)\n",
    "\n",
    "        vocab_size = logits.size(-1)\n",
    "\n",
    "        logits_flat = logits[:, 1:, :].reshape(-1, vocab_size)   # [(batch * (tgt_len-1)), vocab]\n",
    "        targets_flat = tgt_output[:, 1:].reshape(-1)             # [(batch * (tgt_len-1))]\n",
    "\n",
    "        loss = criterion(logits_flat, targets_flat)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt_input, tgt_output in val_loader:\n",
    "            src = src.to(device)\n",
    "            tgt_input = tgt_input.to(device)\n",
    "            tgt_output = tgt_output.to(device)\n",
    "\n",
    "            logits = model(src, tgt_input, teacher_forcing_ratio=TEACHER_FORCING_RATIO_VAL)\n",
    "            vocab_size = logits.size(-1)\n",
    "\n",
    "            logits_flat = logits[:, 1:, :].reshape(-1, vocab_size)\n",
    "            targets_flat = tgt_output[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(logits_flat, targets_flat)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    train_history.append(avg_train_loss)\n",
    "    val_history.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Эпоха {epoch:02d}: train_loss = {avg_train_loss:.4f}, val_loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_without_improve = 0\n",
    "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "        print(f\"  → Новая лучшая модель сохранена (val_loss = {best_val_loss:.4f})\")\n",
    "    else:\n",
    "        epochs_without_improve += 1\n",
    "        print(f\"  → Нет улучшения на валидации {epochs_without_improve}/{PATIENCE} эпох(и)\")\n",
    "\n",
    "        if epochs_without_improve >= PATIENCE:\n",
    "            print(f\"\\nРанняя остановка на эпохе {epoch} — val_loss не улучшается {PATIENCE} эпох подряд.\")\n",
    "            break\n",
    "        \n",
    "if os.path.exists(BEST_MODEL_PATH):\n",
    "    model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "    model.to(device)\n",
    "    print(f\"\\nЗагружена лучшая модель с val_loss = {best_val_loss:.4f}\")\n",
    "else:\n",
    "    print(\"\\nФайл с лучшей моделью не найден — используется последняя версия модели.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21916058",
   "metadata": {},
   "source": [
    "## 7. Инференс: генерация ответов (greedy и beam search)\n",
    "\n",
    "Теперь реализуем:\n",
    "1. Кодирование входной фразы (`encode_sentence`) через энкодер.\n",
    "2. Простейшее жадное декодирование (greedy): на каждом шаге берём argmax.\n",
    "3. Декодирование с `beam search`:\n",
    "   - храним несколько (beam_width) лучших гипотез,\n",
    "   - на каждом шаге расширяем их и выбираем наиболее вероятные последовательности.\n",
    "4. Удобную функцию `generate_reply`, которая позволяет выбрать режим:\n",
    "   - `use_beam=True` — использовать beam search,\n",
    "   - `use_beam=False` — обычное жадное декодирование.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3d5068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(text: str):\n",
    "    model.eval()\n",
    "    cleaned = clean_text(text)\n",
    "    src_ids = text_to_indices(cleaned, word2index, MAX_LEN_SRC,\n",
    "                              add_bos=False, add_eos=False)\n",
    "    src_tensor = torch.tensor(src_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "    src_mask = (src_tensor != PAD_IDX)\n",
    "    return src_tensor, encoder_outputs, hidden, src_mask\n",
    "\n",
    "\n",
    "def greedy_generate(text: str, max_len: int = MAX_LEN_TGT) -> str:\n",
    "    model.eval()\n",
    "    _, encoder_outputs, hidden, src_mask = encode_sentence(text)\n",
    "\n",
    "    generated_ids: list[int] = []\n",
    "    input_step = torch.tensor([BOS_IDX], dtype=torch.long, device=device)  # [1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            logits, hidden, attn_weights = model.decoder(input_step, hidden,\n",
    "                                                         encoder_outputs, src_mask)\n",
    "            next_token = logits.argmax(dim=-1)  # [1]\n",
    "            next_id = next_token.item()\n",
    "            \n",
    "            if next_id == EOS_IDX or next_id == PAD_IDX:\n",
    "                break\n",
    "\n",
    "            generated_ids.append(next_id)\n",
    "            input_step = next_token\n",
    "\n",
    "    return indices_to_text(generated_ids, index2word)\n",
    "\n",
    "\n",
    "def beam_search_generate(text: str, beam_width: int = 3, max_len: int = MAX_LEN_TGT) -> str:\n",
    "    model.eval()\n",
    "    _, encoder_outputs, hidden, src_mask = encode_sentence(text)\n",
    "\n",
    "    beams = [([BOS_IDX], hidden, 0.0)]\n",
    "    completed = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            new_beams = []\n",
    "\n",
    "            for seq_ids, h, log_prob in beams:\n",
    "                last_id = seq_ids[-1]\n",
    "\n",
    "                if last_id == EOS_IDX:\n",
    "                    completed.append((seq_ids, h, log_prob))\n",
    "                    continue\n",
    "\n",
    "                input_step = torch.tensor([last_id], dtype=torch.long, device=device)\n",
    "                logits, new_hidden, attn_weights = model.decoder(input_step, h,\n",
    "                                                                 encoder_outputs, src_mask)\n",
    "\n",
    "                log_probs = torch.log_softmax(logits, dim=-1)  # [1, vocab_size]\n",
    "                topk_log_probs, topk_ids = torch.topk(log_probs, beam_width, dim=-1)\n",
    "\n",
    "                for k in range(beam_width):\n",
    "                    token_id = topk_ids[0, k].item()\n",
    "                    token_log_prob = topk_log_probs[0, k].item()\n",
    "                    new_seq = seq_ids + [token_id]\n",
    "                    new_log_prob = log_prob + token_log_prob\n",
    "                    new_beams.append((new_seq, new_hidden, new_log_prob))\n",
    "\n",
    "            if not new_beams:\n",
    "                break\n",
    "\n",
    "            new_beams.sort(key=lambda x: x[2], reverse=True)\n",
    "            beams = new_beams[:beam_width]\n",
    "\n",
    "        if not completed:\n",
    "            completed = beams\n",
    "\n",
    "    best_seq, _, _ = max(completed, key=lambda x: x[2])\n",
    "\n",
    "    out_ids: list[int] = []\n",
    "    for idx in best_seq[1:]:\n",
    "        if idx in (EOS_IDX, PAD_IDX):\n",
    "            break\n",
    "        out_ids.append(idx)\n",
    "\n",
    "    return indices_to_text(out_ids, index2word)\n",
    "\n",
    "\n",
    "def generate_reply(\n",
    "    text: str,\n",
    "    max_len: int = MAX_LEN_TGT,\n",
    "    use_beam: bool = True,\n",
    "    beam_width: int = 5,\n",
    ") -> str:\n",
    "    if use_beam:\n",
    "        return beam_search_generate(text, beam_width=beam_width, max_len=max_len)\n",
    "    else:\n",
    "        return greedy_generate(text, max_len=max_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4043d364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вход:  привет, как дела?\n",
      "Ответ: хожу в школу. пользователь 1: ладно водитель, вожу пользователь 1: 1: большую? 1: а пользователь 1: ооо\n",
      "\n",
      "Вход:  чем ты занимаешься?\n",
      "Ответ: пользователь 2: 2: да) пользователь я рад вожу пользователь 1: 1: очень очень очень пользователь пользователь 2: приняли\n",
      "\n",
      "Вход:  расскажи о себе\n",
      "Ответ: сходились пользователь 2: я я порисовать области пользователь 1: я это я как?) пользователь 1: это бы пользователь\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"привет, как дела?\",\n",
    "    \"чем ты занимаешься?\",\n",
    "    \"расскажи о себе\",\n",
    "]\n",
    "\n",
    "for sent in test_sentences:\n",
    "    reply = generate_reply(sent, use_beam=True, beam_width=3)\n",
    "    print(f\"Вход:  {sent}\")\n",
    "    print(f\"Ответ: {reply}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64912bc7",
   "metadata": {},
   "source": [
    "## 8. Простой интерактивный чат с моделью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7f18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Введите 'exit' или 'quit', чтобы завершить диалог.\n",
      "Бот: пользователь пользователь 2: 2: пользователь пользователь пользователь 1: пользователь пользователь пользователь пользователь пользователь пользователь пользователь пользователь 2:\n",
      "Чат завершён.\n"
     ]
    }
   ],
   "source": [
    "def chat():\n",
    "    print(\"Введите 'exit' или 'quit', чтобы завершить диалог.\")\n",
    "    while True:\n",
    "        user_message = input(\"Вы: \").strip()\n",
    "        if user_message.lower() in {\"exit\", \"quit\"}:\n",
    "            print(\"Чат завершён.\")\n",
    "            break\n",
    "\n",
    "        bot_reply = generate_reply(user_message, use_beam=True, beam_width=3)\n",
    "        print(f\"Бот: {bot_reply}\")\n",
    "\n",
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e868e268",
   "metadata": {},
   "source": [
    "## 10. Выводы по лабораторной работе\n",
    "\n",
    "В ходе работы было выполнено:\n",
    "1. Загрузка и предобработка диалогового датасета (Toloka Persona Chat, русская версия).\n",
    "2. Формирование пар «реплика → ответ» для обучения Seq2Seq-модели.\n",
    "3. Токенизация текста, построение словаря и подготовка числовых последовательностей.\n",
    "4. Реализация модели Seq2Seq с рекуррентными слоями (GRU) и механизмом внимания.\n",
    "5. Обучение модели на обучающей выборке и оценка на валидации.\n",
    "6. Реализация генерации ответов двумя способами: greedy и beam search.\n",
    "7. Организация простого интерактивного чата с обученной моделью.\n",
    "\n",
    "Возможные направления улучшения:\n",
    "- добавить более мощный токенизатор (например, BPE/SentencePiece),\n",
    "- увеличить размер модели и число эпох обучения,\n",
    "- использовать более сложные варианты внимания или архитектуру Transformer,\n",
    "- экспериментировать с различными стратегиями декодирования (temperature, top-k, nucleus sampling),\n",
    "- добавлять персональные признаки/память для более согласованных диалогов.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscjp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
